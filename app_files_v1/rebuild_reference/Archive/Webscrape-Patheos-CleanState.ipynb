{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.patheos.com/blogs'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(response.content, 'html.parser')\n",
    "soup.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_dict = {}\n",
    "url_dict = []\n",
    "\n",
    "while True:\n",
    "        \n",
    "    for blog in soup.find_all('div', attrs={\"class\":\"col-sm-6\"})[0:20]:\n",
    "        blogurl = blog.find('a')\n",
    "        blog_dict[blog.text] = blogurl['href']\n",
    "        \n",
    "    for urls in blog_dict.values():\n",
    "        print(urls)\n",
    "        if urls:\n",
    "            sub_blog = requests.get(urls)\n",
    "            soup1 = BS(sub_blog.content, 'html.parser')\n",
    "            soup1.prettify\n",
    "            for a in soup1.find_all('a', href=True):\n",
    "                url_dict.append(a['href'])\n",
    "            print(\"FINISHED-\" + urls)    \n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_blogs = list(set(url_dict))\n",
    "prefixes = ('/','#')\n",
    "for word in unique_blogs:\n",
    "    if word.startswith(prefixes):\n",
    "        unique_blogs.remove(word)\n",
    "unique_blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This removes duplicates, since a set can only have unique keys\n",
    "\n",
    "#unique_blogs = list(set(url_dict))\n",
    "#unique_blogs.remove('/privacy-policy')\n",
    "#unique_blogs.remove('/blogs')\n",
    "#unique_blogs.remove('/Topics')\n",
    "#unique_blogs.remove('/subscribe/newsletters')\n",
    "#unique_blogs.remove('/*')\n",
    "\n",
    "#unique_blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subblog_list = []\n",
    "\n",
    "while True:\n",
    "        \n",
    "    for blog_urls in unique_blogs:\n",
    "        print(blog_urls)\n",
    "        if blog_urls:\n",
    "            subsub_blog = requests.get(blog_urls)\n",
    "            soup2 = BS(subsub_blog.content, 'html.parser')\n",
    "            soup2.prettify\n",
    "            for a in soup2.find_all('a', href=True):\n",
    "                subblog_list.append(a['href'])\n",
    "            print(\"FINISHED-\" + blog_urls)    \n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_subblog_list = list(set(subblog_list))\n",
    "unique_subblog_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
