{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from urllib.request import urlopen, URLError\n",
    "from tqdm import tqdm\n",
    "import colorama\n",
    "import parse_tns as tns\n",
    "from sqlalchemy import types, create_engine\n",
    "from pandas.io.sql import SQLTable\n",
    "import cx_Oracle\n",
    "import sys\n",
    "import webscrape_sql_insert_patheos as pinsert\n",
    "import webscrape_pd_scrape_patheos as pscrape\n",
    "import webscrape_compare_current_patheos as pcompare\n",
    "import webscrape_process_results_main as pprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.patheos.com/blogs'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(response.content, 'html.parser')\n",
    "#soup.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_dict = {}\n",
    "url_list1 = []\n",
    "\n",
    "while True:\n",
    "\n",
    "    for blog in soup.find_all('div', attrs={\"class\":\"col-sm-6\"})[0:20]:\n",
    "        blogurl = blog.find('a')\n",
    "        blog_dict[blog.text] = blogurl['href']\n",
    "    i = 0    \n",
    "    for urls in tqdm(blog_dict.values(), desc='Fetch Blogs from Categories'):\n",
    "#        print(urls)\n",
    "        if urls: # and (i < 4):\n",
    "            sub_blog = requests.get(urls)\n",
    "            soup1 = BS(sub_blog.content, 'html.parser')\n",
    "            soup1.prettify\n",
    "            for a in soup1.find_all('a', href=True):\n",
    "                url_list1.append(a['href'])\n",
    "            #print(\"FINISHED \" + urls)\n",
    "            i = i + 1\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(blog_dict)\n",
    "#df_categories_pivot = pd.DataFrame(columns=('category_name','category_url'), index=[0])\n",
    "#print(df_categories)\n",
    "#df_categories = df_categories.set_index([0])\n",
    "df_categories = pd.DataFrame(blog_dict, index=[0])\n",
    "df_categories.reset_index()\n",
    "df_categories = df_categories.T\n",
    "\n",
    "\n",
    "#df_categories.columns = ['beliefs_name','beliefs_url']\n",
    "df_categories.columns\n",
    "#df = pd.concat(df_categories)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'Strip items that start with /, #, www.'\")\n",
    "unique_blogs = list(set(url_list1))\n",
    "prefixes = ('/','#','www.')\n",
    "i = 0\n",
    "for word in unique_blogs:\n",
    "    if word.startswith(prefixes):\n",
    "        unique_blogs.remove(word)\n",
    "#unique_blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remove non-Blog Tirle URLS.\")\n",
    "prefixes2 = ('http://www.patheos.com/blogs/')\n",
    "for word2 in unique_blogs:\n",
    "    if not word2.startswith(prefixes2):\n",
    "        unique_blogs.remove(word2)\n",
    "#unique_blogs\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses .split() to designate the \"/\" as the marker between sections, \n",
    "# which grabs the text after section 4, but does not include section 5.\n",
    "print(\"Fetch just blogs names.\")\n",
    "final_bname = []\n",
    "url_prefix = 'http://www.patheos.com/blogs/'\n",
    "for url_blog in unique_blogs:\n",
    "    url_bname = url_blog.split(\"/\",5)[4:5]\n",
    "    for words in url_bname:\n",
    "        final_bname.append(words)     \n",
    "#final_bname\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Append blog name to 'http://www.patheos.com/blogs/'\")\n",
    "final_url = [url_prefix + x for x in final_bname]\n",
    "#final_url\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Strip out duplicates.\")\n",
    "final_url_dedupe = list(set(final_url))\n",
    "len(final_url_dedupe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_url_dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subblog_list = []\n",
    "\n",
    "while True:\n",
    "    i = 0    \n",
    "    for blog_urls in tqdm(final_url_dedupe, desc='Fetch Post URLs from Blogs (pass 1)'):\n",
    "#        print(blog_urls)\n",
    "        if blog_urls: # and (i < 10):\n",
    "        #if blog_urls:\n",
    "            subsub_blog = requests.get(blog_urls)\n",
    "            soup2 = BS(subsub_blog.content, 'html.parser')\n",
    "            #print(soup2.prettify)\n",
    "            for a in soup2.find_all('a', href=True):\n",
    "                subblog_list.append(a['href'])\n",
    "            #print(\"FINISHED \" + blog_urls)  \n",
    "            i = i + 1\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Remove duplicate blog post urls.\")\n",
    "unique_subblog_list = list(set(subblog_list))\n",
    "#unique_subblog_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_subblog_list)\n",
    "#unique_subblog_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020]\n",
    "final_real_post = []\n",
    "\n",
    "url_real_prefix = 'https://www.patheos.com/blogs/'\n",
    "\n",
    "for post_url in tqdm(unique_subblog_list):\n",
    "    #print(post_url)\n",
    "    for blog_name in final_bname:\n",
    "        test_url = url_real_prefix + blog_name\n",
    "        #i = 2010\n",
    "        #while (i < 2020):\n",
    "        for y in year:\n",
    "            if post_url.startswith(test_url + \"/\" + str(y)):\n",
    "                final_real_post.append(post_url)\n",
    "            #i = i + 1\n",
    "#final_real_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_real_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_posts = list(set(final_real_post))\n",
    "len(individual_posts)\n",
    "\n",
    "comments1 = \"#disqus_thread\"\n",
    "\n",
    "list_without_disqus = []\n",
    "\n",
    "for word3 in individual_posts:\n",
    "    if not word3.endswith(comments1):\n",
    "        #individual_posts.remove(word3)\n",
    "        list_without_disqus.append(word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(individual_posts)\n",
    "len(list_without_disqus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_posts_only = []\n",
    "\n",
    "for word4 in list_without_disqus:\n",
    "    count_list = word4.rsplit(\"/\")\n",
    "    count = len(count_list)\n",
    "    if (count > 8):\n",
    "        list_posts_only.append(word4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_posts_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped_urls = pd.DataFrame(list_posts_only)\n",
    "#df_new_urls.to_csv('patheos_posts.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def access_compare_query(db_user, db_password, user, database, input_df):\n",
    "    connection_string = db_user + '/' + db_password + database\n",
    "    print(\"\\n***CONNECTING***: \" + db_user + '/' + \"************\" + database + \"\\n--------------\")\n",
    "    con = cx_Oracle.connect(connection_string)\n",
    "    df_post_urls = pd.read_sql(\"\"\"SELECT posts_url FROM patheos_posts\"\"\", con)\n",
    "    \n",
    "    df_new_urls = pd.read_csv('patheos_posts.csv')\n",
    "    print(\"New URL list before: \")\n",
    "    print(df_new_urls.shape[0])\n",
    "    \n",
    "    df_post_urls['master'] = 'master'\n",
    "    df_post_urls.set_index('master',append=True,inplace=True)\n",
    "    df_post_urls.rename(columns={'POSTS_URL': 'URL'}, inplace=True)\n",
    "    df_post_urls.sort_values('URL',inplace=True)\n",
    "    #print(df_post_urls)\n",
    "    \n",
    "    df_new_urls = input_df\n",
    "    df_new_urls['scraped'] = 'scraped'\n",
    "    df_new_urls.set_index('scraped',append=True,inplace=True)\n",
    "    df_new_urls.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    df_new_urls.rename(columns={'0': 'URL'}, inplace=True)\n",
    "    df_new_urls.sort_values('URL',inplace=True)\n",
    "    #print(df_new_urls)\n",
    "    \n",
    "    df_merged = df_post_urls.append(df_new_urls, sort=True)\n",
    "    df_merged = df_merged.drop_duplicates().sort_index()\n",
    "    idx = pd.IndexSlice\n",
    "    df_final_list = (df_merged.loc[idx[:, 'scraped'], :])\n",
    "    print(\"New URL list after: \")\n",
    "    print(df_final_list.shape[0])\n",
    "    \n",
    "    test_list = df_final_list.values.tolist()\n",
    "    #for i in test_list:\n",
    "        #print(i)\n",
    "    con.close()\n",
    "    print(\"--------------\\n***DISCONNECTING***\\n\")\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Testing using imported webscrape_pd_scrape_patheos.py\n",
    "\n",
    "def full_scrape(input_list):\n",
    "    invalid_urls = []\n",
    "\n",
    "    patheos_posts = input_list #list_posts_only\n",
    "    df_results = pd.DataFrame(columns=('posts_title','posts_date','posts_author','posts_content','posts_url','blogs_name','blogs_url','beliefs_name','beliefs_url'))\n",
    "\n",
    "    def validate_web_url(url_test):\n",
    "        try:\n",
    "            urlopen(url_test)\n",
    "            return True\n",
    "        except URLError:\n",
    "            return False\n",
    "\n",
    "    total = 0\n",
    "    for url in tqdm(input_list):   \n",
    "        url_str = url[0]\n",
    "        #print(url)\n",
    "        #print(url_str)\n",
    "        if validate_web_url(url_str) == True:\n",
    "            #print(\"- VALID\")\n",
    "            response = requests.get(url_str)\n",
    "            soup = BS(response.content, 'html.parser')\n",
    "            #indiv_post = []\n",
    "            g_name = url_str.split(\"/\", 5)[4:5]\n",
    "            for entry in g_name:\n",
    "                g_blog_name = entry\n",
    "                g_blog_url = \"https://www.patheos.com/blogs/\" + entry\n",
    "            #url_bname = url_blog.split(\"/\",5)[4:5]\n",
    "            g_title = soup.find(\"h1\", {\"class\" : \"entry-title\"})\n",
    "            for g_basic in soup.find_all(\"div\", {\"class\": \"main-post\"}):\n",
    "                g_author = g_basic.find(\"span\", {\"itemprop\": \"author\"})\n",
    "                g_date = g_basic.find(\"span\", {\"itemprop\": \"datePublished dateModified\"})\n",
    "            g_content = soup.find(\"div\", {\"class\": \"story-block\"})\n",
    "            for g_category_info in soup.find_all(\"div\", {\"class\": \"btn btn-xs btn-prime-3 channel-breadcrumb\"}):\n",
    "                #print(g_category_info)\n",
    "                g_category = g_category_info.find(\"a\") # , {\"class\": \"over-dark\"})\n",
    "                g_category_url = \"https:\" + g_category[\"href\"]\n",
    "                g_category_name = g_category.text\n",
    "            if g_title and g_author and g_date and g_content and g_category_name and g_category_url is not None:\n",
    "                df_temp = pd.DataFrame([\n",
    "                                    [g_title.text,     # post title\n",
    "                                     g_date.text,      # post date\n",
    "                                     g_author.text,    # post author\n",
    "                                     g_content.text,   # post content\n",
    "                                     url_str,              # post url\n",
    "                                     g_blog_name,      # blog name\n",
    "                                     g_blog_url,       # blog url\n",
    "                                     g_category_name,  # category name\n",
    "                                     g_category_url]], # category url\n",
    "                                   columns=('posts_title','posts_date','posts_author','posts_content','posts_url','blogs_name','blogs_url','beliefs_name','beliefs_url')\n",
    "                                )\n",
    "                df_results = df_results.append(df_temp, ignore_index=True)\n",
    "                #print(\"- APPENDED\")\n",
    "            else: \n",
    "                #\"- NOT APPENDED\"\n",
    "                invalid_urls.append(url)\n",
    "    df_results = df_results.applymap(lambda x: x.encode('unicode_escape').\n",
    "                 decode('utf-8') if isinstance(x, str) else x)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_results = df_results.applymap(lambda x: x.encode('unicode_escape').\n",
    "                 decode('utf-8') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_results = pd.read_csv('patheos_posts_content.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Testing this using imported webscrape_sql_insert_patheos.py\n",
    "\n",
    "def invalid_category(ecategory):\n",
    "    if invalid_category:\n",
    "        invalid_category.append(ecategory)\n",
    "    else:\n",
    "        invalid_category = []\n",
    "        invalid_cateogey.append(ecategory)\n",
    "        \n",
    "def get_invalid_category():\n",
    "    if invalid_category:\n",
    "        return invalid_category\n",
    "    else:\n",
    "        print(\"No invalid categories found.\")\n",
    "\n",
    "def invalid_blog(eblog):\n",
    "    if invalid_blog:\n",
    "        invalid_blog.append(eblog)\n",
    "    else:\n",
    "        invalid_blog = []\n",
    "        invalid_blog.append(eblog)\n",
    "        \n",
    "def get_invalid_blog():\n",
    "    if invalid_blog:\n",
    "        return invalid_blog\n",
    "    else:\n",
    "        print(\"No invalid blogs found.\")\n",
    "\n",
    "def invalid_post(epost):\n",
    "    if invalid_post:\n",
    "        invalid_post.append(epost)\n",
    "    else:\n",
    "        invalid_post = []\n",
    "        invalid_post.append(epost)\n",
    "\n",
    "def get_invalid_post():\n",
    "    if invalid_post:\n",
    "        return invalid_post\n",
    "    else:\n",
    "        print(\"No invalid posts found.\")\n",
    "\n",
    "#count_row = df.shape[0]\n",
    "#total_process = df_results.shape[0]\n",
    "\n",
    "def access_insert_execute(db_user, db_password, user, database, input_df):\n",
    "    total_process = input_df.shape[0]\n",
    "    invalid_category = []\n",
    "    invalid_blog = []\n",
    "    invalid_post = []\n",
    "    connection_string = db_user + '/' + db_password + database\n",
    "    print(\"\\n***CONNECTING***: \" + db_user + '/' + \"************\" + database + \"\\n--------------\")\n",
    "    con = cx_Oracle.connect(connection_string)\n",
    "    #cur = con.cursor()\n",
    "    con_engine = create_engine('oracle+cx_oracle://' + db_user + ':' + db_password + '@192.168.86.108:5522/?service_name=xe')\n",
    "    #print(con_engine)\n",
    "    \n",
    "    c = 0\n",
    "    ce = 0\n",
    "    b = 0\n",
    "    be = 0\n",
    "    p = 0\n",
    "    pdup = 0\n",
    "    pe = 0\n",
    "    \n",
    "    #for post_url in a(unique_subblog_list):\n",
    "    #enumerate(tqdm(...))\n",
    "    #(1) COMPARE TO EXISTING CATEGORIES:\n",
    "    for i, row in enumerate(input_df.itertuples(), 1):\n",
    "    #for i, row in enumerate(tqdm(df_results.itertuples(), 1)):   \n",
    "        try:\n",
    "            beliefs_test = pd.read_sql(\"SELECT * FROM patheos_beliefs WHERE beliefs_url = '\" + str(row.beliefs_url) + \"'\", con_engine)\n",
    "            if beliefs_test.empty == True:\n",
    "                cur = con.cursor()\n",
    "                cur.execute(\"SELECT MAX(beliefs_number) FROM patheos_beliefs\")\n",
    "                test_number = str(cur.fetchall()[0][0])\n",
    "                if test_number != 'None':\n",
    "                    new_number = int(test_number) + 1\n",
    "                else:\n",
    "                    new_number = 1                \n",
    "                beliefs_number = str(new_number)\n",
    "                cur.execute(\"\"\"INSERT INTO patheos_beliefs\n",
    "                                (BELIEFS_NUMBER, BELIEFS_NAME, BELIEFS_URL)\n",
    "                                VALUES\n",
    "                                ('\"\"\" + str(new_number) + \"\"\"','\"\"\" + row.beliefs_name + \"\"\"','\"\"\" + row.beliefs_url + \"\"\"')\"\"\")\n",
    "                con.commit()\n",
    "                cur2 = con.cursor()\n",
    "                cur2.execute(\"SELECT * FROM patheos_beliefs WHERE beliefs_number = '\" + str(new_number) + \"'\")\n",
    "                res = cur2.fetchall()\n",
    "                cur2.close()\n",
    "                #print(\"INSERTED BELIEF: \" + str(res))\n",
    "                cur.close()\n",
    "                c = c + 1\n",
    "            else:\n",
    "                cur3 = con.cursor()\n",
    "                cur3.execute(\"SELECT MAX(beliefs_number) FROM patheos_beliefs WHERE beliefs_url = '\" + row.beliefs_url + \"'\")\n",
    "                beliefs_number = str(cur3.fetchall()[0][0])\n",
    "                cur3.close()\n",
    "                #print(\"BELIEF ALREADY EXISTS: \" + beliefs_number + \" \" + row.beliefs_name)\n",
    "        except:\n",
    "            invalid_category.append(row.beliefs_url)\n",
    "            #print(\"FAILED TO INSERT \" + row.beliefs_url)\n",
    "            ce = ce + 1 \n",
    "        \n",
    "        \n",
    "    #(2) COMPARE TO EXISTING BLOGS:\n",
    "        try:\n",
    "            blogs_test = pd.read_sql(\"SELECT * FROM patheos_blogs WHERE blogs_url = '\" + str(row.blogs_url) + \"'\", con_engine) \n",
    "                                        # beliefs_number = \" + beliefs_number + \" AND posts_url = '\" + str(row.blogs_url) + \"'\", con_engine)\n",
    "            if blogs_test.empty == True:\n",
    "                cur4 = con.cursor()\n",
    "                cur4.execute(\"SELECT MAX(blogs_number) FROM patheos_blogs\")\n",
    "                test_number2 = str(cur4.fetchall()[0][0])\n",
    "                if test_number2 != 'None':\n",
    "                    new_blog_number = int(test_number2) + 1\n",
    "                else:\n",
    "                    new_blog_number = 1                \n",
    "                blogs_number = str(new_blog_number)\n",
    "                cur4.execute(\"\"\"INSERT INTO patheos_blogs\n",
    "                                (BLOGS_NUMBER, BELIEFS_NUMBER, BLOGS_NAME, BLOGS_URL)\n",
    "                                VALUES\n",
    "                                ('\"\"\" + str(new_blog_number) + \"\"\"','\"\"\" + beliefs_number + \"\"\"','\"\"\" + row.blogs_name + \"\"\"','\"\"\" + row.blogs_url + \"\"\"')\"\"\")           \n",
    "                con.commit()\n",
    "                cur5 = con.cursor()\n",
    "                cur5.execute(\"SELECT * FROM patheos_blogs WHERE blogs_number = '\" + str(new_blog_number) + \"'\")\n",
    "                res2 = cur5.fetchall()\n",
    "                cur5.close()\n",
    "                #print(\"INSERTED BLOG: \" + str(res2))\n",
    "                cur4.close()\n",
    "                b = b + 1\n",
    "            else:\n",
    "                cur6 = con.cursor()\n",
    "                cur6.execute(\"SELECT MAX(blogs_number) FROM patheos_blogs WHERE blogs_url = '\" + row.blogs_url + \"'\")\n",
    "                blogs_number = str(cur6.fetchall()[0][0])\n",
    "                cur6.close()\n",
    "                #print(\"BLOG ALREADY EXISTS: \" + blogs_number + \" \" + row.blogs_name)\n",
    "        except:\n",
    "            invalid_blog.append(row.blogs_url)\n",
    "            #print(\"FAILED TO INSERT \" + row.blogs_url)\n",
    "            be = be + 1\n",
    "        \n",
    "    #(3) COMPARE TO EXISTING POSTS:\n",
    "        try:\n",
    "            posts_test = pd.read_sql(\"SELECT * FROM patheos_posts WHERE posts_url = '\" + str(row.posts_url) + \"'\", con_engine) \n",
    "                                    # beliefs_number = \" + beliefs_number + \" AND posts_url = '\" + str(row.blogs_url) + \"'\", con_engine)\n",
    "            if posts_test.empty == True:\n",
    "                cur7 = con.cursor()\n",
    "                cur7.execute(\"SELECT MAX(posts_number) FROM patheos_posts\")\n",
    "                test_number3 = str(cur7.fetchall()[0][0])\n",
    "                if test_number3 != 'None':\n",
    "                    new_post_number = int(test_number3) + 1\n",
    "                else:\n",
    "                    new_post_number = 1    \n",
    "                var = cur7.var(cx_Oracle.CLOB)\n",
    "                var.setvalue(0, row.posts_content)    # write a small value first to force the temporary LOB to be created    \n",
    "                cur7.execute(\"\"\"INSERT INTO patheos_posts\n",
    "                                (POSTS_NUMBER, POSTS_TITLE, BLOGS_NUMBER, POSTS_AUTHOR, POSTS_DATE, POSTS_CONTENT, POSTS_URL)\n",
    "                                VALUES\n",
    "                                ('\"\"\" + str(new_post_number) + \"\"\"','\"\"\" + row.posts_title + \"\"\"','\"\"\" + blogs_number + \"\"\"','\"\"\" + row.posts_author + \"\"\"',\"\"\" \n",
    "                                      + \"\"\"to_date('\"\"\" + row.posts_date + \"\"\"','MONTH DD, YYYY'), :val ,'\"\"\" + row.posts_url + \"\"\"')\"\"\", val = var)   \n",
    "                                    #to_date('March 13, 2019','MONTH DD, YYYY')\n",
    "                con.commit()\n",
    "                cur8 = con.cursor()\n",
    "                cur8.execute(\"SELECT * FROM patheos_posts WHERE posts_number = '\" + str(new_post_number) + \"'\")\n",
    "                res3 = cur8.fetchall()\n",
    "                cur8.close()\n",
    "                #print(\"INSERTED POST: \" + str(res3) + \"\\n\")\n",
    "                cur7.close()\n",
    "                p = p + 1\n",
    "            else:\n",
    "                cur9 = con.cursor()\n",
    "                cur9.execute(\"SELECT MAX(posts_number) FROM patheos_posts WHERE posts_url = '\" + row.posts_url + \"'\")\n",
    "                posts_number = str(cur9.fetchall()[0][0])\n",
    "                cur9.close()\n",
    "                #print(\"POST ALREADY EXISTS: \" + posts_number + \" \" + row.posts_title + \"\\n\")\n",
    "                pdup = pdup + 1\n",
    "        except:\n",
    "            invalid_post.append(row.posts_url)\n",
    "            #print(\"FAILED TO INSERT \" + row.posts_url)\n",
    "            pe = pe + 1\n",
    "        #print(\"Processed \" + str(i) + \" of \" + str(total_process))\n",
    "        #sys.stdout.write(\"\\033[K\")\n",
    "        sys.stdout.write('\\r'+ str(i) + \"/\" + str(total_process) + \" | BELIEFS: new(\" + str(c) + \") e(\" + str(ce) \n",
    "                                                                 + \") | BLOGS: new(\" + str(b) + \") e(\" + str(be) \n",
    "                                                                 + \") | POSTS: new(\" + str(p) + \") dup(\" + str(pdup) + \") e(\" + str(pe) + \")\")\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "    \n",
    "    #df_data.to_sql('patheos_test', con_engine, if_exists='append',index=False)\n",
    "\n",
    "    print(\"\\nINSERT COMPLETE.\")    \n",
    "    con.close()\n",
    "    print(\"--------------\\n***DISCONNECTING***\\n\")\n",
    "    #return df_user_access"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def empty(value):\n",
    "    try:\n",
    "        value = float(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return bool(value)\n",
    "\n",
    "def f(var):\n",
    "    if isinstance(var, pd.DataFrame):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def process_results(input_result):\n",
    "    if f(input_result) is True:\n",
    "        if (input_result.empty) is False:\n",
    "            print(input_result)\n",
    "            input_result.to_excel('result.xlsx', index=False, header=True)\n",
    "    return input_result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tnsnames_path = \"\"\n",
    "    server = \"PATHEOSDBAPEX\"\n",
    "    df_scraped_urls = pd.DataFrame\n",
    "    environment = tns.parse_tnsnames(tnsnames_path, server)\n",
    "    print(environment)\n",
    "    database_user = \"no_input\" #input(\"Xnumber or Banner Username: \")\n",
    "    database_user = database_user.upper()\n",
    "    #banner_database = environment_select(input(\"Banner database: \"))\n",
    "    #environment_user = (\"Data base user: \")\n",
    "    environment_user = 'system'\n",
    "    environment_password = 'oracle' #getpass.getpass(environment_user + \" Password: \")\n",
    "    temp_list = pprocess.process_results(pcompare.access_compare_query(environment_user, environment_password, database_user, environment, df_scraped_urls))\n",
    "    temp_df = pscrape.full_scrape(temp_list)\n",
    "    pprocess.process_results(pinsert.access_insert_execute(environment_user, environment_password, database_user, environment, temp_df))\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "main()\n",
    "#df_test = main()\n",
    "\n",
    "#df_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TEST New/Old URL Compare\n",
    "\n",
    "def main2():\n",
    "    tnsnames_path = \"C:\\\\Users\\\\Ben\\\\Documents\\\\GitHub\\\\KeePass_Login_App\\\\python_migration\\\\\"\n",
    "    server = \"PATHEOSDBAPEX\"\n",
    "    environment = tns.parse_tnsnames(tnsnames_path, server)\n",
    "    database_user = \"no_input\" #input(\"Xnumber or Banner Username: \")\n",
    "    database_user = database_user.upper()\n",
    "    #banner_database = environment_select(input(\"Banner database: \"))\n",
    "    #environment_user = (\"Data base user: \")\n",
    "    environment_user = 'system'\n",
    "    environment_password = 'oracle' #getpass.getpass(environment_user + \" Password: \")\n",
    "    process_results(access_compare_query(environment_user, environment_password, database_user, environment))\n",
    "    \n",
    "main2()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataframe = df.applymap(lambda x: x.encode('unicode_escape').\n",
    "                 decode('utf-8') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_excel('patheos_posts_content.xlsx')\n",
    "df_results.to_csv('patheos_posts_content.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in invalid urls:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about the unicode_escape:\n",
    "- It will turn single quotes and double quotes into character codes in the excel text\n",
    "- Characters found so far:\n",
    "  - \\u2019: single quotation mark\n",
    "  - \\u201c and u\\201d: left and right double quotation marks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bar = Bar('Processing', max=20)\n",
    "a = 0\n",
    "for i in range(20):\n",
    "    a = a + 1\n",
    "    bar.next()\n",
    "bar.finish()\n",
    "\n",
    "\n",
    "\n",
    "for blog in soup.find_all('div', attrs={\"class\":\"col-sm-6\"})[0:20]:\n",
    "        blogurl = blog.find('a')\n",
    "        blog_dict[blog.text] = blogurl['href']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
