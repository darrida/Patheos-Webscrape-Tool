Step through POC process:

INITIAL PULL:
(1) pull-cats.blogs.#ofposts script pulls categories, blogs, and pages per blogs
(2) pull-posts-every-page script pulls individual post urls for every pages
(4) webscrape_pd_scrape_patheos.py pulls content of individual posts identified by previous step
(5) webscrape_sql_insert_patheos.py inserts content pulled in previous step into sql database tables (3 tables)

UPDATED PULL:
(1) pull-cats.blogs.#ofposts script pulls categories, blogs, and pages per blogs
	- difference: pages per blogs step updates current inventory rather than creates from scratch
(2) pull-posts-every-page script pulls individual post urls
	- difference - existing blogs: if total page count went from 100 to 112, this script will only pull first 13 pages of urls
	- new blogs: will pull all post urls like first step
(3) webscrapt_compare_current_patheos.py performs select statement against database to grab all post urls, and compares against new urls, removes all duplicates
	- key for differences above
(4) webscrape_pd_scrape_patheos.py pulls content of individual posts identified by previous step
(5) webscrape_sql_insert_patheos.py inserts content pulled in previous step into sql database tables (3 tables)


CONSIDERATION:
	- Need to limit number of retrievals that are performed at once.
	- Currently every requests.get() step is proceded by sleep(random.uniform(1,3)), which randomly forces the scraping process to wait between 1 and 3 seconds for each retrieval.
	- I either need to do that all the way through, or, if more is needed, be able to do an additional, longer wait at certain intervals (i.e., after 300 post scrapes, wait 20 seconds).
	- If scrape has to take a break, a mechanism is needed so that the stopping point can be identified so that it can start up where it left off.
	- Should the existing scripts be integrated so that the following happens with each url retrieves, AS it's retrieved:
		- (1) url pulled
		- (2) compare url against existing urls
		- (3) if need, pull content of post
		- (4) sql insert content into the database
		- (5) procede to next url and repeat
		- WHY: It would make it easier to pick up where we left off
	- Should I create a couple of more tables?
		- Scratch table that stores stopping place (cat, blog, last successfully pulled post)
	- Idea about being able to figure out how many new pages exist in a blog since last pull:
		- (1) SELECT statement that pulls number of posts for blog in database
		- (2) Divide number by 10
		- (3) Pull updated blog page number total
		- (4) Subtract step 2 from step 3, difference (rounded up) is number of new pages that need to be scraped (if no difference, then there are anywhere between 0 and 9 new posts, since there is no new page).