{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from datetime import datetime\n",
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import unittest\n",
    "import random\n",
    "import datetime\n",
    "from time import sleep\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import parse_tns as tns\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from pandas.io.sql import SQLTable\n",
    "import cx_Oracle\n",
    "from sqlalchemy import types, create_engine\n",
    "import sqlalchemy\n",
    "#import webscrape_pd_scrape_patheos as pscrape\n",
    "import sys\n",
    "#import database_insert_oracle as oracledb\n",
    "#from passlib.hash import pbkdf2_sha256\n",
    "try:\n",
    "    from configparser import ConfigParser\n",
    "    from configparser import SafeConfigParser\n",
    "except ImportError:\n",
    "    from ConfigParser import ConfigParser  # ver. < 3.0\n",
    "    from ConfigParser import SafeConfigParser\n",
    "    \n",
    "HEADERS = {'user-agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)'\n",
    "                          'AppleWebKit/537.36 (KHTML, like Gecko)'\n",
    "                          'Chrome/73.0.3683.114 Safari/537.36'),\n",
    "                          'referer': 'https://www.patheos.com/blogs/'}\n",
    "\n",
    "import webscrape_sql_insert_patheos as pinsert\n",
    "#import webscrape_pd_scrape_patheos_notqdm as pscrape\n",
    "import webscrape_compare_current_patheos as pcompare\n",
    "import webscrape_process_results_main as pprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WILL BE SEPARATE .py TO  IMPORT\n",
    "def database_connect_oracle(db_user, db_password, database):\n",
    "    sql_connections = []\n",
    "    connection_string = db_user + '/' + db_password + database\n",
    "    print(\"\\n***CONNECTING***: \" + db_user + '/' + \"************\" + database + \"\\n--------------\")\n",
    "    con = \"stand-in string\" #cx_Oracle.connect(connection_string)\n",
    "    sql_connections.append(con)\n",
    "    con_engine = create_engine(\"postgresql://\" + db_user + \":\" + db_password + database)\n",
    "    sql_connections.append(con_engine)\n",
    "    return sql_connections\n",
    "\n",
    "def database_close_oracle(connection, sql_connections):\n",
    "    #connection[0].close()\n",
    "    print(\"--------------\\n***DISCONNECTING***\\n\")\n",
    "\n",
    "def find_blog_pk(url, sql_connections):\n",
    "    con = sql_connections[0]\n",
    "    con_engine = sql_connections[1]\n",
    "    blog_url = url.rsplit(\"/\")\n",
    "    blog_url = '/'.join(blog_url[2:])\n",
    "    blog_number = pd.read_sql(\"SELECT blogs_number FROM patheos_blogs WHERE blogs_url LIKE '%%\" + str(blog_url) + \"'\", con_engine)\n",
    "    if blog_number.empty == False:\n",
    "        for i, row in enumerate(blog_number.itertuples(), 1):\n",
    "            pk = row.blogs_number\n",
    "    else:\n",
    "        pk = 9999\n",
    "    return pk\n",
    "\n",
    "def database_insert_oracle(input_df, sql_connections):\n",
    "    return_list = []\n",
    "    con = sql_connections[0]\n",
    "    con_engine = sql_connections[1]\n",
    "    total_process = input_df.shape[0]\n",
    "    invalid_category = []\n",
    "    invalid_blog = []\n",
    "    invalid_post = []\n",
    "    \n",
    "    c = 0\n",
    "    ce = 0\n",
    "    b = 0\n",
    "    be = 0\n",
    "    p = 0\n",
    "    pdup = 0\n",
    "    pe = 0\n",
    "    \n",
    "    #(1) COMPARE TO EXISTING CATEGORIES:\n",
    "    for i, row in enumerate(input_df.itertuples(), 1): \n",
    "        try:\n",
    "            #set_trace()\n",
    "            belief_url = row.beliefs_url.rsplit(\"/\")\n",
    "            belief_url = '/'.join(belief_url[2:])\n",
    "            belief_url = belief_url.replace('%','%%')\n",
    "            beliefs_test = pd.read_sql(\"SELECT * FROM patheos_beliefs WHERE beliefs_url LIKE '%%\" + str(belief_url) + \"'\", con_engine)\n",
    "            if beliefs_test.empty == True:\n",
    "                with con_engine.connect() as cur:\n",
    "                    test_number = cur.execute(\"SELECT MAX(beliefs_number) FROM patheos_beliefs\").fetchall()[0][0]\n",
    "                    if test_number != None:\n",
    "                        new_number = int(test_number) + 1\n",
    "                    else:\n",
    "                        new_number = 1                \n",
    "                    beliefs_number = str(new_number)\n",
    "                    cur.execute(sqlalchemy.text(\"\"\"INSERT INTO patheos_beliefs\n",
    "                                    (BELIEFS_NUMBER, BELIEFS_NAME, BELIEFS_URL)\n",
    "                                    VALUES\n",
    "                                    ('\"\"\" + str(new_number) + \"\"\"','\"\"\" + row.beliefs_name + \"\"\"','\"\"\" + row.beliefs_url + \"\"\"')\"\"\"))\n",
    "                with con_engine.connect() as cur2:\n",
    "                    res = cur2.execute(\"SELECT * FROM patheos_beliefs WHERE beliefs_number = '\" + str(new_number) + \"'\").fetchall()[0][0]\n",
    "                c = c + 1\n",
    "            else:\n",
    "                with con_engine.connect() as cur3:\n",
    "                    beliefs_number = str(cur3.execute(\"SELECT MAX(beliefs_number) FROM patheos_beliefs WHERE beliefs_url LIKE '%%\" + belief_url + \"'\").fetchall()[0][0])\n",
    "        except Exception as e:\n",
    "            set_trace()\n",
    "            ErrorLogEntry = str(datetime.now()) + ' BeliefStepError: ' + str(repr(e)) + ' | URL: {}'.format(row.beliefs_url)\n",
    "            with open(r'webscrape_patheos_error_log.csv','a', newline='') as fd:\n",
    "                fd.write(str(ErrorLogEntry) + '\\r')\n",
    "            invalid_category.append(row.beliefs_url)\n",
    "            ce = ce + 1 \n",
    "        \n",
    "        return_list.append(beliefs_number)\n",
    "    #(2) COMPARE TO EXISTING BLOGS:\n",
    "        try:\n",
    "            blog_url = row.blogs_url.rsplit(\"/\")\n",
    "            blog_url = '/'.join(blog_url[2:])\n",
    "            blog_url = blog_url.replace('%','%%')\n",
    "            blogs_test = pd.read_sql(\"SELECT * FROM patheos_blogs WHERE blogs_url LIKE '%%\" + str(blog_url) + \"'\", con_engine) \n",
    "            if blogs_test.empty == True:\n",
    "                with con_engine.connect() as cur4:\n",
    "                    test_number2 = cur4.execute(\"SELECT MAX(blogs_number) FROM patheos_blogs\").fetchall()[0][0]\n",
    "                    if test_number2 != None:\n",
    "                        new_blog_number = int(test_number2) + 1\n",
    "                    else:\n",
    "                        new_blog_number = 1                \n",
    "                    blogs_number = str(new_blog_number)\n",
    "                    cur4.execute(sqlalchemy.text(\"\"\"INSERT INTO patheos_blogs\n",
    "                                    (BLOGS_NUMBER, BELIEFS_NUMBER, BLOGS_NAME, BLOGS_URL)\n",
    "                                    VALUES\n",
    "                                    ('\"\"\" + str(new_blog_number) + \"\"\"','\"\"\" + beliefs_number + \"\"\"','\"\"\" + row.blogs_name + \"\"\"','\"\"\" + row.blogs_url + \"\"\"')\"\"\"))\n",
    "                with con_engine.connect() as cur5:\n",
    "                    res2 = cur5.execute(\"SELECT * FROM patheos_blogs WHERE blogs_number = '\" + str(new_blog_number) + \"'\").fetchall()[0][0]\n",
    "                b = b + 1\n",
    "            else:\n",
    "                with con_engine.connect() as cur6:\n",
    "                    blogs_number = str(cur6.execute(\"SELECT MAX(blogs_number) FROM patheos_blogs WHERE blogs_url LIKE '%%\" + blog_url + \"'\").fetchall()[0][0])\n",
    "        except Exception as e:\n",
    "            set_trace()\n",
    "            ErrorLogEntry = str(datetime.now()) + ' BlogStepError: ' + str(repr(e)) + ' | URL: {}'.format(row.blogs_url)\n",
    "            with open(r'webscrape_patheos_error_log.csv','a', newline='') as fd:\n",
    "                fd.write(str(ErrorLogEntry) + '\\r')\n",
    "            invalid_blog.append(row.blogs_url)\n",
    "            be = be + 1\n",
    "        return_list.append(blogs_number)\n",
    "    #(3) COMPARE TO EXISTING POSTS:\n",
    "        try:\n",
    "            post_url = row.posts_url.rsplit(\"/\")\n",
    "            post_url = '/'.join(post_url[2:])\n",
    "            #set_trace()\n",
    "            post_url = post_url.replace('%','%%')\n",
    "            posts_test = pd.read_sql(\"SELECT * FROM patheos_posts WHERE posts_url LIKE '%%\" + str(post_url) + \"'\", con_engine) \n",
    "            if posts_test.empty == True:\n",
    "                with con_engine.connect() as cur7:\n",
    "                    test_number3 = cur7.execute(\"SELECT MAX(posts_number) FROM patheos_posts\").fetchall()[0][0]\n",
    "                    if test_number3 != None:\n",
    "                        new_post_number = int(test_number3) + 1\n",
    "                    else:\n",
    "                        new_post_number = 1 \n",
    "                    posts_number = str(new_post_number)\n",
    "                    posts_title = row.posts_title.replace(':','\\\\:')\n",
    "                    posts_title = row.posts_title.replace(\"'\",\"\\u2018\")\n",
    "                    posts_author = row.posts_author.replace(':','\\\\:')\n",
    "                    posts_author = row.posts_author.replace(\"'\",\"\\u2018\")\n",
    "                    posts_content = row.posts_content.replace(':','\\\\:')\n",
    "                    posts_content = posts_content.replace(\"'\",\"\\u2018\")\n",
    "                    posts_content = posts_content.replace(\"_\",\"\\\\_\")\n",
    "                    cur7.execute(sqlalchemy.text(\"\"\"INSERT INTO patheos_posts\n",
    "                                    (POSTS_NUMBER, POSTS_TITLE, BLOGS_NUMBER, POSTS_AUTHOR, POSTS_DATE, POSTS_CONTENT, POSTS_URL)\n",
    "                                    VALUES\n",
    "                                    ('\"\"\" + str(new_post_number) + \"\"\"','\"\"\" + posts_title + \"\"\"','\"\"\" + blogs_number + \"\"\"','\"\"\" + posts_author + \"\"\"',\"\"\" \n",
    "                                          + \"\"\"to_date('\"\"\" + row.posts_date + \"\"\"','MONTH DD, YYYY'),'\"\"\" + posts_content + \"\"\"','\"\"\" + row.posts_url + \"\"\"')\"\"\"))\n",
    "                    p = p + 1\n",
    "                return_code = 1\n",
    "            else:\n",
    "                with con_engine.connect() as cur9:\n",
    "                    posts_number = str(cur9.execute(\"SELECT MAX(posts_number) FROM patheos_posts WHERE posts_url LIKE '%%\" + post_url + \"'\").fetchall()[0][0])\n",
    "                    pdup = pdup + 1\n",
    "                    \n",
    "                return_code = 2\n",
    "        except Exception as e:\n",
    "            set_trace()\n",
    "            ErrorLogEntry = str(datetime.now()) + ' PostStepError: ' + str(repr(e)) + ' | URL: {}'.format(row.posts_url)\n",
    "            with open(r'webscrape_patheos_error_log.csv','a', newline='') as fd:\n",
    "                fd.write(str(ErrorLogEntry) + '\\r')\n",
    "            invalid_post.append(row.posts_url)\n",
    "            pe = pe + 1\n",
    "            return_code = 0\n",
    "            posts_number = -1\n",
    "            \n",
    "        return_list.append(posts_number) #2\n",
    "        return_list.append(c)       #3 beliefs new\n",
    "        return_list.append(ce)      #4 beliefs error\n",
    "        return_list.append(b)       #5 blogs new\n",
    "        return_list.append(be)      #6 blogs error\n",
    "        return_list.append(p)       #7 posts new\n",
    "        return_list.append(pdup)    #8 posts duplicate\n",
    "        return_list.append(pe)      #9 posts error\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_system_status_values(file, section, option, value):\n",
    "    config = ConfigParser()\n",
    "    config.read(file)\n",
    "    cfgfile = open(file, 'w')\n",
    "    if config.has_section(section) == True:\n",
    "        config.set(section, option, value)\n",
    "    else:\n",
    "        config.add_section(section)\n",
    "        config.set(section, option, value)\n",
    "    config.write(cfgfile)\n",
    "    cfgfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists_config_file(file, section, option):\n",
    "    config = ConfigParser()\n",
    "    config.read(file)\n",
    "    if config.has_option(section, option) == True:\n",
    "        page_number = int(config.get(section, option))\n",
    "    else:\n",
    "        page_number = 1\n",
    "    return page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_point_config_file(file):\n",
    "    config = ConfigParser()\n",
    "    config.read(file)\n",
    "    return_save = []\n",
    "    if config.has_option('SAVE POINT', 'Category') == True:\n",
    "        category = config.get('SAVE POINT', 'Category')\n",
    "        return_save.append(category)\n",
    "    else:\n",
    "        category = \"none\"\n",
    "    if config.has_option('SAVE POINT', 'Blog') == True:\n",
    "        blog = config.get('SAVE POINT', 'Blog')\n",
    "        return_save.append(blog)\n",
    "    else:\n",
    "        blog = \"none\"\n",
    "    if config.has_option('SAVE POINT', 'Blog-Key') == True:\n",
    "        blog_key = config.get('SAVE POINT', 'Blog-Key')\n",
    "        return_save.append(blog_key)\n",
    "    else:\n",
    "        blog_key = \"none\"\n",
    "    if config.has_option('SAVE POINT', 'Page') == True:\n",
    "        page = int(config.get('SAVE POINT', 'Page'))\n",
    "        return_save.append(page)\n",
    "    else:\n",
    "        page = 0\n",
    "           # list contains: [category, blog, blog_key, page]\n",
    "    return return_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patheos_urls_from_page(url, index=None):\n",
    "    url_test = requests.get(url)\n",
    "    if url_test != \"<Response [404]>\":\n",
    "        soup2 = BS(url_test.content, 'html.parser')\n",
    "        if index == None:\n",
    "            post_urls = []\n",
    "            for blog[0] in soup2.find_all('h2', attrs={\"class\":\"entry-title\"}):\n",
    "                blog_url1 = blog.find('a')\n",
    "                blog_url2 = blog_url1['href']\n",
    "                post_urls.append(blog_url2)\n",
    "        else:\n",
    "            for blog[int(index)] in soup2.find_all('h2', attrs={\"class\":\"entry-title\"}):\n",
    "                blog_url1 = blog.find('a')\n",
    "                blog_url2 = blog_url1['href']\n",
    "                post_urls = blog_url2\n",
    "    else:\n",
    "        if index == None:\n",
    "            post_urls = []\n",
    "        else:\n",
    "            post_urls = ''\n",
    "    return post_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_blogs():\n",
    "    url = 'http://www.patheos.com/blogs'\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.content, 'html.parser')\n",
    "\n",
    "    blog_list = []\n",
    "\n",
    "    for blog in soup.find_all('div', attrs={\"class\":\"related-content clearfix related-content-sm decorated channel-list\"}):\n",
    "        blog_url1 = blog.find('a')\n",
    "        blog_url2 = blog_url1['href']\n",
    "        blog_list.append(blog_url2) \n",
    "\n",
    "    blog_lists = []\n",
    "    \n",
    "    for i in blog_list:\n",
    "        split_url = i.rsplit(\"/\")\n",
    "        blog_name = split_url[3]\n",
    "        blog_lists.append(blog_name)\n",
    "    \n",
    "    blog_dict = {i:[] for i in blog_lists}\n",
    "\n",
    "    blog_cat_prefix = \"https://www.patheos.com/\"\n",
    "\n",
    "    i = 0    \n",
    "    for blog_urls in tqdm(blog_dict, desc='Fetch blog urls for each category'):\n",
    "        if blog_urls:\n",
    "            query_url = blog_cat_prefix + blog_urls\n",
    "            #sleep(random.uniform(1, 3))\n",
    "            subsub_blog = requests.get(query_url)\n",
    "            soup2 = BS(subsub_blog.content, 'html.parser')\n",
    "            for blog in soup2.find_all('div', attrs={\"class\":\"author-info\"}):\n",
    "                for blog_url0 in blog.find_all('div', attrs={\"class\":\"title\"}):\n",
    "                    blog_url1 = blog_url0.find('a')\n",
    "                    blog_url2 = blog_url1['href']\n",
    "                    blog_dict[blog_urls].append(blog_url2)\n",
    "\n",
    "            i = i + 1\n",
    "        else:\n",
    "            continue\n",
    "    return blog_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_blog_dict(blog_dict, sql_connections):\n",
    "    df_results = pd.DataFrame(columns=('pk','category','blog_url'))\n",
    "    df_hold = pd.DataFrame(columns=('pk','category','blog_url'))\n",
    "    for c in tqdm(blog_dict, desc='Fetching primary keys for blogs'):\n",
    "        for b in blog_dict[c]:\n",
    "            g_category = c\n",
    "            g_blog_url = b\n",
    "            blog_pk = find_blog_pk(b, sql_connections) #oracledb.find_blog_pk(b, sql_connections)\n",
    "            g_blog_pk = str(blog_pk)\n",
    "            if g_category and g_blog_url is not None:\n",
    "                if blog_pk != 9999:\n",
    "                    df_temp = pd.DataFrame([[g_blog_pk,      # blog db pk\n",
    "                                             g_category,     # blog name\n",
    "                                             g_blog_url]],    # blog url\n",
    "                                            columns=('pk','category','blog_url'))\n",
    "                    df_results = df_results.append(df_temp)\n",
    "                else:\n",
    "                    df_temp = pd.DataFrame([[g_blog_pk,\n",
    "                                             g_category,\n",
    "                                             g_blog_url]],\n",
    "                                            columns=('pk','category','blog_url'))\n",
    "                    df_hold = df_hold.append(df_temp)\n",
    "    # I'm banking on this adding the new entries to the bottom of the list and \n",
    "    # adding index numbers that are higher than the rest of the entries. These are\n",
    "    # not what the pk for these will end up being, but they will allow them to sit\n",
    "    # further down on the list than the save point location (i.e., they won't get missed)\n",
    "    df_results = df_results.append(df_hold, ignore_index=True)\n",
    "    #print(df_results.to_string())\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_404(blog, page_number, increment, name):\n",
    "    pn = 0\n",
    "    return_str = page_number\n",
    "    continue_on = True\n",
    "    if page_number < 1:\n",
    "        pn = increment\n",
    "        return_str = 0\n",
    "    else:\n",
    "        pn = page_number + increment    \n",
    "    blog_test = requests.get(blog, timeout=5)\n",
    "    blog_url = blog_test.url\n",
    "    while continue_on == True:\n",
    "        blog_page_url = blog_url + \"page/\" + str(pn)\n",
    "        if pn < 20000:\n",
    "            try:\n",
    "                url_test = requests.get(blog_page_url, timeout=5) #, allow_redirects=True)\n",
    "                if url_test.status_code != 404:\n",
    "                    sys.stdout.write('\\r' + blog + \" | page(\" + str(pn) + \")\") # | category total: (\" + str(t) + \")\")\n",
    "                    return_str = pn\n",
    "                    pn = pn + increment\n",
    "                else:\n",
    "                    continue_on = False                \n",
    "            except:\n",
    "                return_str = -1\n",
    "                continue_on = False\n",
    "        else:\n",
    "            return_str = -2\n",
    "            continue_on = False\n",
    "    if increment == 1:\n",
    "        sys.stdout.write('\\r' + blog + \" | page(\" + str(return_str) + \")\") # | category total: (\" + str(t) + \")\")        \n",
    "    return return_str                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blog_find_pages(blogname_list, list_name):\n",
    "    c = 0\n",
    "    t = 1\n",
    "    b = 1\n",
    "    url_list = []\n",
    "    continue_on = True\n",
    "    i = blogname_list\n",
    "    try:\n",
    "        continue_on = True\n",
    "        page_prefix = \"/page/\"\n",
    "        blog_name = i.rsplit(\"/\")\n",
    "        try:\n",
    "            p = int(exists_config_file('patheos_blogs_pages.ini', list_name, blog_name[4]))\n",
    "        except:\n",
    "            throw_away = 0\n",
    "        if not p:\n",
    "            p = 0\n",
    "        try:\n",
    "            p1 = test_404(i, p, 500, blog_name[4])\n",
    "        except:\n",
    "            throw_away = 0\n",
    "\n",
    "        try:\n",
    "            sys.stdout.write('\\r' + list_name.upper() + \" | category number: \" + str(b) + \"/\" + str(len(blogname_list)))\n",
    "        except:\n",
    "            throw_away = 0\n",
    "\n",
    "        if ((p1 != -1) and (p1 != -2)):\n",
    "            p2 = test_404(i, p1, 100, blog_name[4])\n",
    "            p = p1\n",
    "            if p2 != -1:\n",
    "                p3 = test_404(i, p2, 50, blog_name[4])\n",
    "                p = p2\n",
    "                if p3 != -1:\n",
    "                    p4 = test_404(i, p3, 25, blog_name[4])\n",
    "                    p = p4\n",
    "\n",
    "        if ((p != -1) and (p != -2)):\n",
    "            while continue_on == True:    \n",
    "                sys.stdout.write('\\r' + list_name.upper() + \" | category number: \" + str(b) + \"/\" + str(len(blogname_list)))\n",
    "                p = test_404(i, p, 1, blog_name[4])\n",
    "                continue_on = False\n",
    "            t = t + p\n",
    "            b = b + 1\n",
    "            update_system_status_values('patheos_blogs_pages.ini', list_name, blog_name[4], str(p))\n",
    "        else:\n",
    "            update_system_status_values('patheos_blogs_pages.ini', list_name, blog_name[4], str(p))\n",
    "            b = b + 1\n",
    "            sys.stdout.write('\\r' + list_name.upper() + \" | category number: \" + str(b) + \"/\" + str(len(blogname_list)))\n",
    "    except:\n",
    "        p = -1\n",
    "        update_system_status_values('patheos_blogs_pages.ini', list_name, blog_name[4], str(p))\n",
    "        sys.stdout.write('\\r' + list_name.upper() + \" | category number: \" + str(b) + \"/\" + str(len(blogname_list)))\n",
    "    return str(p)\n",
    "\n",
    "fail_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_dict_list(blog_dict, file):\n",
    "    df_lists_results = pd.DataFrame()\n",
    "    \n",
    "    blog_posts = {i:[] for i in blog_dict}\n",
    "    for i in blog_dict:\n",
    "        result_list = fetch_page_posts(i, blog_dict[i], file)\n",
    "        blog_posts[i].append(result_list)\n",
    "    return blog_posts #dictionary\n",
    "\n",
    "def fetch_page_posts(blog_dict, file, connection_info, csv=None, inserttest=None): #(dictionary, string, list)\n",
    "    cwd = os.getcwd()\n",
    "    c = 0\n",
    "    ce = 0\n",
    "    b = 0\n",
    "    be = 0\n",
    "    p = 0\n",
    "    pdup = 0\n",
    "    pe = 0\n",
    "    blog_number = 0\n",
    "    url_list = []\n",
    "    url_errors = []\n",
    "    continue_on = True\n",
    "    page_prefix = \"page/\"\n",
    "    ############ NEED TO PROVIDE VARIABLES\n",
    "    db_user = connection_info[0]\n",
    "    db_password = connection_info[1]\n",
    "    # user = connection_info[2]\n",
    "    database = connection_info[2]\n",
    "    sql_connections = database_connect_oracle(db_user, db_password, database) #oracledb.database_connect_oracle(db_user, db_password, user, database)\n",
    "    print(sql_connections)\n",
    "    ############\n",
    "    if csv == None:\n",
    "        input_df = convert_blog_dict(blog_dict, sql_connections)\n",
    "        input_df.pk = input_df.pk.astype(int)\n",
    "        input_df = input_df.sort_values(by=['pk','category','blog_url'])\n",
    "        input_df = input_df.reset_index(drop=True)\n",
    "        input_df = input_df[input_df.category != 'contemplative-blogs']\n",
    "        input_df.to_csv('sorted_blog_list.csv')\n",
    "    if csv == True:\n",
    "        input_df = pd.read_csv('sorted_blog_list.csv')\n",
    "    \n",
    "    # RESTORE SAVE POINT PROCESS\n",
    "    save_point_list = save_point_config_file('patheos_save_point-testing.ini')\n",
    "    print(save_point_list)\n",
    "    save_key = 0\n",
    "    if save_point_list:\n",
    "        if save_point_list[3] != 0:\n",
    "            save_category = save_point_list[0]\n",
    "            save_blog = save_point_list[1]\n",
    "            save_key = save_point_list[2]\n",
    "            save_page = save_point_list[3]\n",
    "            index_point = int(save_key)\n",
    "    else:\n",
    "        index_point = 0\n",
    "        save_key = 0\n",
    "        save_page = 1\n",
    "\n",
    "    pull_page = save_page\n",
    "    second_loop = False\n",
    "    \n",
    "    if (index_point > 0) and (int(save_page) > 0):\n",
    "        index_point = index_point - 1\n",
    "        print(index_point)\n",
    "        input_df = input_df[index_point:]\n",
    "    for i, row in enumerate(input_df.itertuples(), 0): #, row in islice(input_df.itertuples(), save_key, None):\n",
    "        total_pages = blog_find_pages(row.blog_url, row.category)\n",
    "        blog_name = row.blog_url.rsplit(\"/\")\n",
    "        blog_test = requests.get(row.blog_url, timeout=5)\n",
    "        blog_url = blog_test.url\n",
    "        tposts = 0\n",
    "        if second_loop == True:\n",
    "            pull_page = 1\n",
    "        continue_blog = True\n",
    "        while continue_blog == True:\n",
    "            blog_page_url = blog_url + page_prefix + str(pull_page)\n",
    "\n",
    "            url_test = requests.get(blog_page_url)\n",
    "            if str(url_test) != \"<Response [404]>\":\n",
    "                soup2 = BS(url_test.content, 'html.parser')\n",
    "                for blog in soup2.find_all('h2', attrs={\"class\":\"entry-title\"}):\n",
    "                    blog_url1 = blog.find('a')\n",
    "                    blog_url2 = blog_url1['href']\n",
    "                    temp_list = []\n",
    "                    temp_list.append(blog_url2)\n",
    "                    sleep(random.uniform(0, 2))\n",
    "                    ############\n",
    "                    #df_post_content = pscrape.full_scrape(temp_list, \"yes\")\n",
    "                    df_post_content = full_scrape(temp_list, \"yes\")\n",
    "                    ############\n",
    "                    if df_post_content.empty == True:\n",
    "                        set_trace()\n",
    "                        with open(r'empty_dataframe_errors.csv','a', newline='') as fd:\n",
    "                            fd.write(blog_url2 + '\\r')\n",
    "                        blog_number = -2\n",
    "                        pe = pe + 1\n",
    "                    else:\n",
    "                        if inserttest == True:\n",
    "                            #df_post_content = pd.read_csv('df_test.csv')\n",
    "                            df_post_content = pd.read_csv('df_test.csv')\n",
    "                        return_pk = database_insert_oracle(df_post_content, sql_connections) #oracledb.database_insert_oracle(df_post_content, sql_connections)\n",
    "                        ############\n",
    "                        # return_pk CONTAINS THE FOLLOW LIST (by list index location)\n",
    "                        #0 categories primary key\n",
    "                        blog_number = return_pk[1] #1 blogs primary key\n",
    "                        #2 posts primary key\n",
    "                        c = c + return_pk[3] #3 beliefs new\n",
    "                        ce = c + return_pk[4] #4 beliefs error\n",
    "                        b = b + return_pk[5] #5 blogs new\n",
    "                        be = be + return_pk[6] #6 blogs error\n",
    "                        p = p + return_pk[7] #7 posts new\n",
    "                        pdup = pdup + return_pk[8] #8 posts duplicate\n",
    "                        if return_pk[8] == 1:\n",
    "                            with open(r'duplicate_post_reported.csv','a', newline='') as fd:\n",
    "                                fd.write(blog_url2 + '\\r')\n",
    "                        pe = pe + return_pk[9] #9 posts error\n",
    "                        ############\n",
    "                        if return_pk[2] == -1:\n",
    "                            with open(r'insert_url_errors.csv','a', newline='') as fd:\n",
    "                                fd.write(blog_url2 + '\\r')\n",
    "                        else:\n",
    "                            # UPDATES SAVE POINT FILE WITH CURRENT WORKING LOCATION\n",
    "                            update_system_status_values('patheos_save_point-testing.ini', 'SAVE POINT', 'Category', row.category)\n",
    "                            update_system_status_values('patheos_save_point-testing.ini', 'SAVE POINT', 'Blog', blog_name[4])\n",
    "                            update_system_status_values('patheos_save_point-testing.ini', 'SAVE POINT', 'Blog-Key', str(return_pk[1]))\n",
    "                            update_system_status_values('patheos_save_point-testing.ini', 'SAVE POINT', 'Page', str(pull_page))\n",
    "\n",
    "                    url_list.append(blog_url2)\n",
    "                    tposts += 1\n",
    "                    sys.stdout.write('\\r' + row.category.upper() + \": \" + blog_name[4] \n",
    "                                          + \" | blog number: \" + str(blog_number)\n",
    "                                          + \" | page: \" + str(pull_page) + \"/\" + str(total_pages)\n",
    "                                          + \" | posts: \" + str(tposts)\n",
    "                                          + \" | BELIEFS: new(\" + str(c) + \") e(\" + str(ce) \n",
    "                                          + \") | BLOGS: new(\" + str(b) + \") e(\" + str(be)\n",
    "                                          + \") | POSTS: new(\" + str(p) + \") dup(\" + str(pdup) + \") e(\" + str(pe) + \")\")\n",
    "                pull_page = pull_page + 1\n",
    "                sys.stdout.write('\\r' + row.category.upper() + \": \" + blog_name[4] \n",
    "                                          + \" | blog number: \" + str(blog_number)\n",
    "                                          + \" | page: \" + str(pull_page) + \"/\" + str(total_pages)\n",
    "                                          + \" | posts: \" + str(tposts)\n",
    "                                          + \" | BELIEFS: new(\" + str(c) + \") e(\" + str(ce) \n",
    "                                          + \") | BLOGS: new(\" + str(b) + \") e(\" + str(be)\n",
    "                                          + \") | POSTS: new(\" + str(p) + \") dup(\" + str(pdup) + \") e(\" + str(pe) + \")\")\n",
    "            else:\n",
    "                continue_blog = False\n",
    "                second_loop = True\n",
    "            sys.stdout.write('\\r' + row.category.upper() + \": \" + blog_name[4] \n",
    "                                          + \" | blog number: \" + str(blog_number)\n",
    "                                          + \" | page: \" + str(pull_page) + \"/\" + str(total_pages)\n",
    "                                          + \" | posts: \" + str(tposts)\n",
    "                                          + \" | BELIEFS: new(\" + str(c) + \") e(\" + str(ce) \n",
    "                                          + \") | BLOGS: new(\" + str(b) + \") e(\" + str(be)\n",
    "                                          + \") | POSTS: new(\" + str(p) + \") dup(\" + str(pdup) + \") e(\" + str(pe) + \")\")\n",
    "        sys.stdout.write('\\r' + row.category.upper() + \": \" + blog_name[4] \n",
    "                                          + \" | blog number: \" + str(blog_number)\n",
    "                                          + \" | page: \" + str(pull_page) + \"/\" + str(total_pages)\n",
    "                                          + \" | posts: \" + str(tposts)\n",
    "                                          + \" | BELIEFS: new(\" + str(c) + \") e(\" + str(ce) \n",
    "                                          + \") | BLOGS: new(\" + str(b) + \") e(\" + str(be)\n",
    "                                          + \") | POSTS: new(\" + str(p) + \") dup(\" + str(pdup) + \") e(\" + str(pe) + \")\")\n",
    "    \n",
    "        df_temp = pd.DataFrame({'Post URL':url_list})\n",
    "        csv_name = blog_name[4] + '.csv'\n",
    "        df_temp.to_csv(cwd + \"\\\\blog_url_lists\\\\\" + csv_name)\n",
    "\n",
    "    ############ NEED TO PROVIDE VARIABLES\n",
    "    print(sql_connections)\n",
    "    database_close_oracle(sql_connections) #oracledb.database_close_oracle(sql_connections)\n",
    "    ############\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_connection_list(db_user, db_password, database):\n",
    "    connection_info_list = []\n",
    "    connection_info_list.append(db_user)\n",
    "    connection_info_list.append(db_password)\n",
    "    #connection_info_list.append(user)\n",
    "    connection_info_list.append(database)\n",
    "    return connection_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen, URLError\n",
    "import requests\n",
    "\n",
    "def full_scrape(input_list, unicode_escape_yes_no):\n",
    "    try:\n",
    "        invalid_urls = []\n",
    "\n",
    "        patheos_posts = input_list #list_posts_only\n",
    "        df_results = pd.DataFrame(columns=('posts_title','posts_date','posts_author','posts_content','posts_url','blogs_name','blogs_url','beliefs_name','beliefs_url'))\n",
    "\n",
    "        def validate_web_url(url_test):\n",
    "            try:\n",
    "                #print(url_test)\n",
    "                urlopen(url_test)\n",
    "                return True\n",
    "            except URLError:\n",
    "                return False\n",
    "\n",
    "        total = 0\n",
    "        #set_trace()\n",
    "        for url in input_list:   \n",
    "            #print(url)\n",
    "            url_str = url #[0]\n",
    "            #print(url)\n",
    "            #print(url_str)\n",
    "            if validate_web_url(url_str) == True:\n",
    "                #print(\"- VALID\")\n",
    "                response = requests.get(url_str)\n",
    "                soup = BS(response.content, 'html.parser')\n",
    "                #indiv_post = []\n",
    "                g_name = url_str.split(\"/\", 5)[4:5]\n",
    "                for entry in g_name:\n",
    "                    g_blog_name = entry\n",
    "                    g_blog_url = \"https://www.patheos.com/blogs/\" + entry\n",
    "                #url_bname = url_blog.split(\"/\",5)[4:5]\n",
    "                g_title = soup.find(\"h1\", {\"class\" : \"entry-title\"})\n",
    "                for g_basic in soup.find_all(\"div\", {\"class\": \"main-post\"}):\n",
    "                    if g_basic.find(\"span\", {\"itemprop\": \"author\"}):\n",
    "                        g_author = g_basic.find(\"span\", {\"itemprop\": \"author\"})\n",
    "                    if g_basic.find(\"span\", {\"itemprop\": \"datePublished dateModified\"}):\n",
    "                        g_date = g_basic.find(\"span\", {\"itemprop\": \"datePublished dateModified\"})\n",
    "                g_content = soup.find(\"div\", {\"class\": \"story-block\"})\n",
    "                for g_category_info in soup.find_all(\"div\", {\"class\": \"btn btn-xs btn-prime-3 channel-breadcrumb\"}):\n",
    "                    #print(g_category_info)\n",
    "                    g_category = g_category_info.find(\"a\") # , {\"class\": \"over-dark\"})\n",
    "                    g_category_url = \"https:\" + g_category[\"href\"]\n",
    "                    if g_category_url == \"https:\" + g_category[\"href\"]:\n",
    "                        g_category_name = g_category.text\n",
    "                if g_title and g_author and g_date and g_content and g_category_name and g_category_url is not None:\n",
    "                    df_temp = pd.DataFrame([\n",
    "                                        [g_title.text,     # post title\n",
    "                                         g_date.text,      # post date\n",
    "                                         g_author.text,    # post author\n",
    "                                         g_content.text,   # post content\n",
    "                                         url_str,              # post url\n",
    "                                         g_blog_name,      # blog name\n",
    "                                         g_blog_url,       # blog url\n",
    "                                         g_category_name,  # category name\n",
    "                                         g_category_url]], # category url\n",
    "                                       columns=('posts_title','posts_date','posts_author','posts_content','posts_url','blogs_name','blogs_url','beliefs_name','beliefs_url')\n",
    "                                    )\n",
    "                    df_results = df_results.append(df_temp, ignore_index=True)\n",
    "                    #print(\"- APPENDED\")\n",
    "                else: \n",
    "                    #\"- NOT APPENDED\"\n",
    "                    invalid_urls.append(url)\n",
    "        if unicode_escape_yes_no == \"yes\":\n",
    "            df_results = df_results.applymap(lambda x: x.encode('unicode_escape').\n",
    "                     decode('utf-8') if isinstance(x, str) else x)\n",
    "\n",
    "        if df_results.empty == True:\n",
    "            ErrorLogEntry = str(datetime.now()) + ' full_scrape() empty dataframe error | URL: {}'.format(url_str)\n",
    "            with open(r'webscrape_patheos_error_log.csv','a', newline='') as fd:\n",
    "                fd.write(str(ErrorLogEntry) + '\\r')\n",
    "        return df_results\n",
    "    except Exception as e:\n",
    "        set_trace()\n",
    "        ErrorLogEntry = str(datetime.now()) + ' full_scrape() error: ' + str(repr(e)) + ' | URL: {}'.format(url_str)\n",
    "        with open(r'webscrape_patheos_error_log.csv','a', newline='') as fd:\n",
    "            fd.write(str(ErrorLogEntry) + '\\r')\n",
    "\n",
    "\n",
    "# Notes about the unicode_escape:\n",
    "# - It will turn single quotes and double quotes into character codes in the excel text\n",
    "# - Characters found so far:\n",
    "#   - \\u2019: single quotation mark\n",
    "#   - \\u201c and u\\201d: left and right double quotation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_full_url_scrape():\n",
    "    # TNSNAMES INFO\n",
    "    tnsnames_path = \"\"\n",
    "    server = \"LOCALPOSTGRES\"\n",
    "    database_str = tns.parse_tnsnames(tnsnames_path, server)\n",
    "    print(database_str)\n",
    "    # GATHER OTHER CONNECTION INFO\n",
    "    #database_user = \"no_input\".upper() #input(\"Xnumber or Banner Username: \")\n",
    "    database_user = 'postgres'\n",
    "    database_password = 'password' #getpass.getpass(environment_user + \" Password: \")\n",
    "    connection_info = build_connection_list(database_user, database_password, database_str)\n",
    "    # SWITCH TO EASILY ENABLE TEST MODE\n",
    "    prod = False\n",
    "    if prod == True:\n",
    "        blog_dict = fetch_blogs()\n",
    "    else:\n",
    "        blog_dict = {}\n",
    "    if prod == True:\n",
    "        fetch_page_posts(blog_dict, 'patheos_blogs_pages.ini', connection_info)\n",
    "    else:\n",
    "        fetch_page_posts(blog_dict, 'patheos_blogs_pages.ini', connection_info, csv=True)\n",
    "    \n",
    "main_full_url_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch_page_posts(blog_dict, 'patheos_blogs_pages.ini', connection_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
